setwd("~/Desktop/DSA/MachineLearning/cap12/12r")
library(readr)
library(qdap)
library(tm)
library(RWeka)
library(wordcloud)
library(plotrix)
library(ggthemes)
library(ggplot2)
# Carrega os dados
6
df_amazon <- read_csv("dados/amazon.csv")
df_google <- read_csv("dados/google.csv")
amazon_pros <- df_amazon$pros
amazon_cons <- df_amazon$cons
google_pros <- df_google$pros
google_cons <- df_google$cons
func_limpa_texto <- function(x){
x <- na.omit(x)
x <- replace_abbreviation(x)
x <- replace_contraction(x)
x <- replace_number(x)
x <- replace_ordinal(x)
x <- replace_symbol(x)
x <- tolower(x)
return(x)
}
amazon_pros <- func_limpa_texto(amazon_pros)
amazon_cons <- func_limpa_texto(amazon_cons)
google_pros <- func_limpa_texto(google_pros)
google_cons <- func_limpa_texto(google_cons)
amazon_p_corp <- VCorpus(VectorSource(amazon_pros))
amazon_c_corp <- VCorpus(VectorSource(amazon_cons))
google_p_corp <- VCorpus(VectorSource(google_pros))
google_c_corp <- VCorpus(VectorSource(google_cons))
func_limpa_corpus <- function(x){
x <- tm_map(x,removePunctuation)
x <- tm_map(x,stripWhitespace)
x <- tm_map(x,removeWords, c(stopwords("en"), "Amazon", "Google", "Company"))
return(x)
}
amazon_pros_corp <- func_limpa_corpus(amazon_p_corp)
amazon_cons_corp <- func_limpa_corpus(amazon_c_corp)
google_pros_corp <- func_limpa_corpus(google_p_corp)
google_cons_corp <- func_limpa_corpus(google_c_corp)
tokenizer <- function(x)
NGramTokenizer(x, Weka_control(min = 2, max = 2))
amazon_p_tdm    <- TermDocumentMatrix(amazon_pros_corp)
amazon_p_tdm_m  <- as.matrix(amazon_p_tdm)
amazon_p_freq   <- rowSums(amazon_p_tdm_m)
amazon_p_f.sort <- sort(amazon_p_freq, decreasing = TRUE)
barplot(amazon_p_freq[1:5])
amazon_p_tdm    <- TermDocumentMatrix(amazon_pros_corp, control = list(tokenize=tokenizer))
amazon_p_tdm_m  <- as.matrix(amazon_p_tdm)
amazon_p_freq   <- rowSums(amazon_p_tdm_m)
amazon_p_f.sort <- sort(amazon_p_freq,decreasing = TRUE)
df_amazon_p <- data.frame(term = names(amazon_p_f.sort), num = amazon_p_f.sort)
View(df_amazon_p)
wordcloud(df_amazon_p$term,
df_amazon_p$num,
max.words = 100,
color = "tomato4")
amazon_c_tdm    <- TermDocumentMatrix(amazon_cons_corp, control = list(tokenize = tokenizer))
amazon_c_tdm_m  <- as.matrix(amazon_c_tdm)
amazon_c_freq   <- rowSums(amazon_c_tdm_m)
amazon_c_f.sort <- sort(amazon_c_freq, decreasing = TRUE)
df_amazon_c <- data.frame(term = names(amazon_c_f.sort), num = amazon_c_f.sort)
View(df_amazon_c)
wordcloud(df_amazon_c$term,
df_amazon_c$num,
max.words = 100,
color = "palevioletred")
amazon_c_tdm <- TermDocumentMatrix(amazon_cons_corp,control = list(tokenize = tokenizer))
amazon_c_tdm <- removeSparseTerms(amazon_c_tdm, 0.993)
amazon_c_hclust <- hclust(dist(amazon_c_tdm, method = "euclidean"), method = "complete")
plot(amazon_c_hclust)
amazon_p_tdm    <- TermDocumentMatrix(amazon_pros_corp, control = list(tokenize=tokenizer))
amazon_p_m      <- as.matrix(amazon_p_tdm)
amazon_p_freq   <- rowSums(amazon_p_m)
token_frequency <- sort(amazon_p_freq,decreasing = TRUE)
token_frequency[1:5]
findAssocs(amazon_p_tdm, "fast paced", 0.2)
all_google_pros   <- paste(df_google$pros, collapse = "")
all_google_cons   <- paste(df_google$cons, collapse = "")
all_google        <- c(all_google_pros,all_google_cons)
all_google_clean  <- func_limpa_texto(all_google)
all_google_vs     <- VectorSource(all_google_clean)
all_google_vc     <- VCorpus(all_google_vs)
all_google_clean2 <- func_limpa_corpus(all_google_vc)
all_google_tdm    <- TermDocumentMatrix(all_google_clean2)
colnames(all_google_tdm) <- c("Google Pros", "Google Cons")
all_google_tdm_m <- as.matrix(all_google_tdm)
comparison.cloud(all_google_tdm_m, colors = c("orange", "blue"), max.words = 50)
amazon_pro    <- paste(df_amazon$pros, collapse = "")
google_pro    <- paste(df_google$pros, collapse = "")
all_pro       <- c(amazon_pro, google_pro)
all_pro_clean <- func_limpa_texto(all_pro)
all_pro_vs    <- VectorSource(all_pro)
all_pro_vc    <- VCorpus(all_pro_vs)
all_pro_corp  <- func_limpa_corpus(all_pro_vc)
tdm.bigram = TermDocumentMatrix(all_pro_corp,control = list(tokenize = tokenizer))
colnames(tdm.bigram) <- c("Amazon", "Google")
tdm.bigram <- as.matrix(tdm.bigram)
common_words <- subset(tdm.bigram, tdm.bigram[,1] > 0 & tdm.bigram[,2] > 0 )
difference <- abs(common_words[, 1] - common_words[,2])
common_words <- cbind(common_words, difference)
common_words <- common_words[order(common_words[,3],decreasing = TRUE),]
top25_df <- data.frame(x = common_words[1:25,1],
y = common_words[1:25,2],
labels = rownames(common_words[1:25,]))
pyramid.plot(top25_df$x,
top25_df$y,
labels=top25_df$labels,
gap=15,
top.labels=c("Amazon Pros", "Vs", "Google Pros"),
unit = NULL,
main = "Palavras em Comum")
amazon_cons    <- paste(df_amazon$cons, collapse = "")
google_cons    <- paste(df_google$cons, collapse = "")
all_cons       <- c(amazon_cons,google_cons)
all_cons_clean <- func_limpa_texto(all_cons)
all_cons_vs    <- VectorSource(all_cons)
all_cons_vc    <- VCorpus(all_cons_vs)
all_cons_corp  <- func_limpa_corpus(all_cons_vc)
tdm.cons_bigram = TermDocumentMatrix(all_cons_corp,control=list(tokenize =tokenizer))
colnames(tdm.cons_bigram) <- c("Amazon", "Google")
tdm.cons_bigram <- as.matrix(tdm.cons_bigram)
common_words <- subset(tdm.cons_bigram, tdm.cons_bigram[,1] > 0 & tdm.cons_bigram[,2] > 0 )
difference <- abs(common_words[, 1] - common_words[,2])
common_words <- cbind(common_words, difference)
common_words <- common_words[order(common_words[,3], decreasing = TRUE),]
top25_df <- data.frame(x = common_words[1:25,1],
y = common_words[1:25,2],
labels = rownames(common_words[1:25,]))
pyramid.plot(top25_df$x,
top25_df$y,
labels=top25_df$labels,
gap=10,
top.labels = c("Amazon Cons","Vs","Google Cons"),
unit = NULL,
main = "Palavras em Comum")
tdm.unigram <- TermDocumentMatrix(all_pro_corp)
colnames(tdm.unigram) <- c("Amazon","Google")
tdm.unigram <- as.matrix(tdm.unigram)
commonality.cloud(tdm.unigram, colors = c("tomato2", "yellow2"), max.words = 100)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm.bigram <- TermDocumentMatrix(all_pro_corp,control = list(tokenize=BigramTokenizer))
colnames(tdm.bigram) <- c("Amazon", "Google")
tdm.bigram <- as.matrix(tdm.bigram)
commonality.cloud(tdm.bigram, colors = c("tomato2", "yellow2"), max.words = 100)
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm.trigram <- TermDocumentMatrix(all_pro_corp,control = list(tokenize=TrigramTokenizer))
colnames(tdm.trigram) <- c("Amazon","Google")
tdm.trigram <- as.matrix(tdm.trigram)
commonality.cloud(tdm.trigram, colors = c("tomato2", "yellow2"), max.words = 100)
amazon_tdm <- TermDocumentMatrix(amazon_p_corp)
associations <- findAssocs(amazon_tdm,"fast",0.2)
associations_df <- list_vect2df(associations)[,2:3]
ggplot(associations_df, aes(y = associations_df[,1])) +
geom_point(aes(x = associations_df[,2]),
data = associations_df, size = 3) +
theme_gdocs()
google_tdm <- TermDocumentMatrix(google_c_corp)
associations <- findAssocs(google_tdm,"fast",0.2)
associations_df <- list_vect2df(associations)[,2:3]
ggplot(associations_df,aes(y=associations_df[,1])) +
geom_point(aes(x = associations_df[,2]),
data = associations_df, size = 3) +
theme_gdocs()
ggplot(associations_df,aes(y=associations_df[,1])) +
geom_point(aes(x = associations_df[,2]),
data = associations_df, size = 3) +
theme_gdocs()
tokenizer <- function(x)
NGramTokenizer(x, Weka_control(min = 2, max = 2))
barplot(amazon_p_freq[1:5])
comparison.cloud(all_google_tdm_m, colors = c("orange", "blue"), max.words = 50)
pwd
